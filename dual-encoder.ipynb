{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91b5835bd9f44713bfc86f998c60c883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb4b55c44c9a479da9a46b919688f03b",
              "IPY_MODEL_3c6e1a8de0564a9e86fca2107d765d92",
              "IPY_MODEL_ddf467b087db421b992d3d71b96e4ff4"
            ],
            "layout": "IPY_MODEL_a89a82316ddf41f6b7eef05bbc820e5a"
          }
        },
        "fb4b55c44c9a479da9a46b919688f03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93cdc46d8ac4f65983ab1638b113bbd",
            "placeholder": "​",
            "style": "IPY_MODEL_3ddb5d7c8a37425e940624e9e35989b5",
            "value": "config.json: 100%"
          }
        },
        "3c6e1a8de0564a9e86fca2107d765d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92da8904dbc54fb59e0ba9e2bd597aa9",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_718f6371fc374346897ebdc86fc40d01",
            "value": 483
          }
        },
        "ddf467b087db421b992d3d71b96e4ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1480d5e4664bcba60ff67fa7fd61fd",
            "placeholder": "​",
            "style": "IPY_MODEL_fc18ad0ac4944773a88145045326741c",
            "value": " 483/483 [00:00&lt;00:00, 33.5kB/s]"
          }
        },
        "a89a82316ddf41f6b7eef05bbc820e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93cdc46d8ac4f65983ab1638b113bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ddb5d7c8a37425e940624e9e35989b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92da8904dbc54fb59e0ba9e2bd597aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "718f6371fc374346897ebdc86fc40d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef1480d5e4664bcba60ff67fa7fd61fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc18ad0ac4944773a88145045326741c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e58c357bd8414728b0a1a66f9ce74d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77bc8fec33144b609c33188719daff27",
              "IPY_MODEL_4d656ae254de4e1698ae26a8b0a079db",
              "IPY_MODEL_d1b5aa6da1d94d4aa4fa6d54235fbf3a"
            ],
            "layout": "IPY_MODEL_9c7dc5946bae4b8db158867190551a51"
          }
        },
        "77bc8fec33144b609c33188719daff27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1d28bc73514404487729a45653617ef",
            "placeholder": "​",
            "style": "IPY_MODEL_6e44a098508b457fba71e61652c6770a",
            "value": "model.safetensors: 100%"
          }
        },
        "4d656ae254de4e1698ae26a8b0a079db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42f82f8aac34feaa33bc4539cc2718c",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69a47a9ce763441b90248747fcae3bb2",
            "value": 267954768
          }
        },
        "d1b5aa6da1d94d4aa4fa6d54235fbf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61baef8d221a48a48ee6e7ff622b0283",
            "placeholder": "​",
            "style": "IPY_MODEL_3d56c012fb0c442ca5078d1a89e5fe5a",
            "value": " 268M/268M [00:05&lt;00:00, 49.7MB/s]"
          }
        },
        "9c7dc5946bae4b8db158867190551a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d28bc73514404487729a45653617ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e44a098508b457fba71e61652c6770a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e42f82f8aac34feaa33bc4539cc2718c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69a47a9ce763441b90248747fcae3bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61baef8d221a48a48ee6e7ff622b0283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d56c012fb0c442ca5078d1a89e5fe5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "411a73599dfb405e91767d590fac2d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d58c5254f67f445181989aa3d33c23e1",
              "IPY_MODEL_44e51c944fb541d8b21710b045fddbc2",
              "IPY_MODEL_66321011660f4226afb424567238b509"
            ],
            "layout": "IPY_MODEL_f40432e01efb4730b6b3d046f9d6b8cf"
          }
        },
        "d58c5254f67f445181989aa3d33c23e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee53bf14339241bbb1a9b7a8cd91cee2",
            "placeholder": "​",
            "style": "IPY_MODEL_37032783bee04bd8b39f82ca09abd4ba",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "44e51c944fb541d8b21710b045fddbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2996b5f67a634cf8b1e1b5f76020ac00",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5a39f786f4a454bb51aa190b193dc0d",
            "value": 28
          }
        },
        "66321011660f4226afb424567238b509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64578aab57c4839b3daccdd802002e4",
            "placeholder": "​",
            "style": "IPY_MODEL_4e11485052c14ff98bbc05bacc733cda",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.83kB/s]"
          }
        },
        "f40432e01efb4730b6b3d046f9d6b8cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee53bf14339241bbb1a9b7a8cd91cee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37032783bee04bd8b39f82ca09abd4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2996b5f67a634cf8b1e1b5f76020ac00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a39f786f4a454bb51aa190b193dc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a64578aab57c4839b3daccdd802002e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e11485052c14ff98bbc05bacc733cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb37b61cf4da4f598d6f63382991c3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a992b3355b14c7d8690dbde109d4afc",
              "IPY_MODEL_4884760c58fa4e2b8d849576064eca4f",
              "IPY_MODEL_1242ec1eec42461cb76c068a1645ebde"
            ],
            "layout": "IPY_MODEL_ea6b4e1c709448fa975b4334c1d9ca3e"
          }
        },
        "4a992b3355b14c7d8690dbde109d4afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83618b47b9b84fcfa58fe7e7a1ca3c7d",
            "placeholder": "​",
            "style": "IPY_MODEL_aca93d46f3a943e989f5782a141116ce",
            "value": "vocab.txt: 100%"
          }
        },
        "4884760c58fa4e2b8d849576064eca4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d72ca8f7dc31443b94c22f6433e8dab2",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b846d804c4241babc29bbc376764c69",
            "value": 231508
          }
        },
        "1242ec1eec42461cb76c068a1645ebde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cdd4535aa2a415db2caac1ad5aadae0",
            "placeholder": "​",
            "style": "IPY_MODEL_273a8912f3fd4d25950c4d825df9409a",
            "value": " 232k/232k [00:00&lt;00:00, 7.02MB/s]"
          }
        },
        "ea6b4e1c709448fa975b4334c1d9ca3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83618b47b9b84fcfa58fe7e7a1ca3c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca93d46f3a943e989f5782a141116ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d72ca8f7dc31443b94c22f6433e8dab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b846d804c4241babc29bbc376764c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cdd4535aa2a415db2caac1ad5aadae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273a8912f3fd4d25950c4d825df9409a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "413c037d882f4e0cbf66e305d545e398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a23afda0bb3145ff84be0ec49591da47",
              "IPY_MODEL_63d04267dc52473fa3a40aee1bbddeea",
              "IPY_MODEL_613b0cbf87334f0ea17d3f381a2b9a8f"
            ],
            "layout": "IPY_MODEL_dfb915f9e93c474c8b75ee09c93c3aa8"
          }
        },
        "a23afda0bb3145ff84be0ec49591da47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bac45bbb5b6485b8d754e0a168fcfe0",
            "placeholder": "​",
            "style": "IPY_MODEL_d33fb990dffd4e80b64708ec9b0a755d",
            "value": "tokenizer.json: 100%"
          }
        },
        "63d04267dc52473fa3a40aee1bbddeea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f6496ddb134771ade1471f7d76fb19",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2416180d124b32a316a2e21eb50278",
            "value": 466062
          }
        },
        "613b0cbf87334f0ea17d3f381a2b9a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5977c8f8f8024ea6bf7497e02fc4c385",
            "placeholder": "​",
            "style": "IPY_MODEL_33dd8241c5d9446eb979a94c8e1b022a",
            "value": " 466k/466k [00:00&lt;00:00, 31.6MB/s]"
          }
        },
        "dfb915f9e93c474c8b75ee09c93c3aa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bac45bbb5b6485b8d754e0a168fcfe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d33fb990dffd4e80b64708ec9b0a755d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7f6496ddb134771ade1471f7d76fb19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2416180d124b32a316a2e21eb50278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5977c8f8f8024ea6bf7497e02fc4c385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33dd8241c5d9446eb979a94c8e1b022a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import some useful packages and set everything up.\n",
        "\n",
        "(Remember to set the hardware accelerator to GPU in Notebook settings.)"
      ],
      "metadata": {
        "id": "lBts0MGy1aa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZLyQaG7PTWqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d9d5f8-c3dd-4306-a9de-7c9d272da679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "!pip install transformers\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "import unittest\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "transformers.logging.set_verbosity_error()  # Disable needless warnings\n",
        "\n",
        "def set_seed(seed):  # For reproducibility, fix random seeds.\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tip*: Google Colab is not really efficient in collecting memory from the GPU. If we create a model and put it on the GPU repeatedly it runs into out-of-memory problems. Use the function below to clear the model from the GPU memory whenever that happens."
      ],
      "metadata": {
        "id": "UwpiBPvq11ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "def clear_from_gpu(thing):  # E.g., clear_from_gpu(model)\n",
        "  with torch.no_grad():\n",
        "    thing = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "913G9t2h7Bn5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "f9Ry17LuMUuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a769c40-fc81-474d-9f39-706f23a9e0d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Questions (NQ) Dataset"
      ],
      "metadata": {
        "id": "cNt596m9UmiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download [a preprocessed version of NQ](https://drive.google.com/file/d/1rHm8Nt3nIkmOeO2VU2YQPqdAoV0WQoMM/view?usp=share_link). We will assume that we have extracted the files (nq-train.json and nq-val.json) to the directory data/nq-toy/ in our Google Drive account. Let's load the data and stare at it.  "
      ],
      "metadata": {
        "id": "AS5MmiukUkA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount. You will have to authorize this operation.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T_ddoyDFVX1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993ed48b-2b43-4307-c3c0-00e771349704"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/My Drive/data/nq-toy/nq-train.json') as f:\n",
        "  data_train_all = json.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/data/nq-toy/nq-val.json') as f:\n",
        "  data_val_all = json.load(f)\n",
        "\n",
        "print('{:d} training and {:d} validation examples'.format(len(data_train_all), len(data_val_all)))\n",
        "print('Each example has the following attributes: ' + str(list(data_train_all[0].keys())))\n",
        "\n",
        "set_seed(42)\n",
        "for example in random.sample(data_train_all, 5):\n",
        "  print('\\nQ: ' + example['question'])\n",
        "  print('A: ' + str(example['answers']))\n",
        "  print('C: ' + str(example['gold_context']))"
      ],
      "metadata": {
        "id": "HHBLfSsdViPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74241cf2-e799-4e48-f681-994a85fc64b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58880 training and 6515 validation examples\n",
            "Each example has the following attributes: ['question', 'answers', 'gold_context']\n",
            "\n",
            "Q: who plays killer croc in the movie suicide squad\n",
            "A: ['Adewale Akinnuoye - Agbaje']\n",
            "C: {'title': 'Suicide Squad (film)', 'text': 'the film, and Scott Eastwood announced that he had been cast. Later that month, it was confirmed that Adewale Akinnuoye-Agbaje and Karen Fukuhara had been cast as Killer Croc and Katana, respectively. Adam Beach, Ike Barinholtz, and Jim Parrack were added to the cast in April 2015. In January 2016, Ben Affleck was confirmed to reprise his role as Batman from \"\". Filming began on April 13, 2015. On April 26 and 27, filming was to take place at the Hy\\'s Steakhouse. A \"snowstorm\" scene was filmed on April 29 on the Adelaide St. and in Ching Lane. On May'}\n",
            "\n",
            "Q: who is cate blanchett 's character in lord of the rings\n",
            "A: ['Galadriel']\n",
            "C: {'title': 'Galadriel', 'text': 'Annette Crosbie in Ralph Bakshi\\'s 1978 animated film of \"The Lord of the Rings\", and by Marian Diamond in BBC Radio\\'s 1981 serialisation. While she did not appear in the 1980 animated \"The Return of the King\", she was mentioned by name when Frodo refers to Galadriel\\'s phial. In Peter Jackson\\'s \"Lord of the Rings\" trilogy and \"The Hobbit\" prequel trilogy, Galadriel is played by Cate Blanchett. In the movies, other than possessing the Ring Nenya, Galadriel displays an ability to communicate with others telepathically and to assume a radiant and terrifying appearance. In \"\", as she was present at'}\n",
            "\n",
            "Q: what kind of joint is the si joint\n",
            "A: ['a synovial plane joint']\n",
            "C: {'title': 'Sacroiliac joint', 'text': 'Sacroiliac joint The sacroiliac joint or SI joint (SIJ) is the joint between the sacrum and the ilium bones of the pelvis, which are connected by strong ligaments. In humans, the sacrum supports the spine and is supported in turn by an ilium on each side. The joint is strong, supporting the entire weight of the upper body. It is a synovial plane joint with irregular elevations and depressions that produce interlocking of the two bones. The human body has two sacroiliac joints, one on the left and one on the right, that often match each other but are highly'}\n",
            "\n",
            "Q: how old was fabregas when he joined arsenal\n",
            "A: ['16']\n",
            "C: {'title': 'Cesc Fàbregas', 'text': 'Cesc Fàbregas Francesc \"Cesc\" Fàbregas Soler (, ; born 4 May 1987) is a Spanish professional footballer who plays as a central midfielder for club Chelsea and the Spain national team. Fàbregas came through \"La Masia\", Barcelona\\'s youth academy, leaving at 16 when he was signed by Premier League club Arsenal in September 2003. Following injuries to key midfielders in the early part of the 2004–05 season, he went on establish himself in the team. He broke several of the club\\'s records in the process, earning a reputation as one of the best players in his position, and won the'}\n",
            "\n",
            "Q: which treaty required germany to pay reparations after world war\n",
            "A: ['Treaty of Versailles']\n",
            "C: {'title': 'World War I reparations', 'text': 'requirement to pay reparations as the \"chief battleground of the post-war era\" and \"the focus of the power struggle between France and Germany over whether the Versailles Treaty was to be enforced or revised\". The Treaty of Versailles (signed in 1919) and the 1921 London Schedule of Payments required Germany to pay 132 billion gold marks in reparations to cover civilian damage caused during the war. This figure was divided into three categories of bonds: A, B, and C. Of these, Germany was required to pay towards \\'A\\' and \\'B\\' bonds totaling 50 billion marks () unconditionally. The payment of'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NQ consists of real-world Google queries (in question form), each manually labeled with a valid answer (possibly multiple answers, but mostly just one) and a piece of text from Wikipedia (~110 tokens), aka. a \"context\", from which the answer can be found and justified. This is a subset of NQ with short answers (prepared by [this paper](https://arxiv.org/pdf/1906.00300.pdf)).\n",
        "\n",
        "Read the [original NQ paper](https://aclanthology.org/Q19-1026.pdf) about the excruciating care the authors took to ensure data quality (e.g., 5-way annotations for evaluation data)."
      ],
      "metadata": {
        "id": "siS9VxTUYvoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducing Data"
      ],
      "metadata": {
        "id": "PO2EuuGOySne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though NQ is not a large dataset, for our purposes it will be too slow to train models if we use the whole dataset. Thus we will reduce the data size."
      ],
      "metadata": {
        "id": "1PkQMGuDyXD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "num_samples_train = 500\n",
        "num_samples_val = 1000\n",
        "data_train = random.sample(data_train_all, num_samples_train)\n",
        "data_val = random.sample(data_val_all, num_samples_val)"
      ],
      "metadata": {
        "id": "ztuauk5Cy_6t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Base (KB)"
      ],
      "metadata": {
        "id": "itc-TDy9XDP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider open-domain question answering (ODQA) in which the KB consists of all passages (aka., contexts, text blocks) in Wikpedia, around 20 million. For obvious computational reasons we will use a small toy KB that consists only of the gold passages in the portion of NQ that we use."
      ],
      "metadata": {
        "id": "XCTv_It4XCR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KB = []\n",
        "for example in data_train + data_val:\n",
        "  KB.append({'title': example['gold_context']['title'], 'text': example['gold_context']['text']})\n",
        "print(len(KB))"
      ],
      "metadata": {
        "id": "JsL8eHSlb4aI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523af41a-3675-4342-fe30-846f7ce556d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dual Encoder"
      ],
      "metadata": {
        "id": "HYg_O9AfqhFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to retrieve the right passage for a given question from the KB by training a dual encoder. The dual encoder has a trivial architecture:\n",
        "\n",
        "1. Query encoder gets $Q \\in \\mathbb{R}^{B \\times T}$ representing $B$ questions of length $T$ (assuming padding). It encodes it into $X \\in \\mathbb{R}^{B \\times d}$ (i.e., $d$-dimensional question embeddings).\n",
        "\n",
        "2. Passage encoder gets $P \\in \\mathbb{R}^{B' \\times T'}$ representing $B'$ passages of length $T'$. It encodes it into $Y \\in \\mathbb{R}^{B' \\times d'}$."
      ],
      "metadata": {
        "id": "1pzA39dgqi1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualEncoder(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, query_encoder, passage_encoder):\n",
        "    super().__init__()\n",
        "    self.query_encoder = query_encoder\n",
        "    self.passage_encoder = passage_encoder\n",
        "\n",
        "  def forward(self, Q=None, Q_mask=None, P=None, P_mask=None):\n",
        "    X = self.query_encoder(Q, Q_mask) if Q is not None else None\n",
        "    Y = self.passage_encoder(P, P_mask) if P is not None else None\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "D6zESjEdlUYL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are free to choose whatever encoders we like. We will use [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert), which is a distilled version of [BERT](https://arxiv.org/pdf/1810.04805.pdf) with \"only\" 66 million parameters. We will take the embedding of the [CLS] token which is prepended to every input sequence as a single-vector representation of the sequence (if you don't know what [CLS] is, you need to review BERT).\n"
      ],
      "metadata": {
        "id": "YrcK0Gtoro4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DistilBertModel"
      ],
      "metadata": {
        "id": "lDI1LA7dmerL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistilBertEncoder(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None):\n",
        "    # Passing the input_ids and attention_mask through the encoder to get the sequence of hidden states.\n",
        "    outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
        "    # Extracting the embeddings for the [CLS] token (i.e.the first token) from each sequence in the batch.\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "    return cls_embeddings"
      ],
      "metadata": {
        "id": "uInA__XNllBS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEncoder(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    self.input_ids = torch.LongTensor([[101, 1001, 1002, 1003, 1004], [101, 207, 350, 394, 999], [101, 10000, 10000, 10000, 10000]])\n",
        "    self.places = 4\n",
        "\n",
        "  def test_encoder(self):\n",
        "    model = DistilBertEncoder()\n",
        "    embeddings = model(self.input_ids)\n",
        "    self.assertEqual(embeddings.dim(), 2)\n",
        "    self.assertEqual(embeddings.size(0), 3)\n",
        "    self.assertEqual(embeddings.size(1), 768)  # This is the hidden dimension of DistilBERT\n",
        "    self.assertAlmostEqual(embeddings[0, 0].item(), 0.2982, places=self.places)\n",
        "    self.assertAlmostEqual(embeddings[1, 0].item(), -0.0705, places=self.places)\n",
        "    self.assertAlmostEqual(embeddings[2, 0].item(), 0.1151, places=self.places)\n",
        "\n",
        "unittest.main(TestEncoder(), argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "LvKJrmCfkBU7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "91b5835bd9f44713bfc86f998c60c883",
            "fb4b55c44c9a479da9a46b919688f03b",
            "3c6e1a8de0564a9e86fca2107d765d92",
            "ddf467b087db421b992d3d71b96e4ff4",
            "a89a82316ddf41f6b7eef05bbc820e5a",
            "c93cdc46d8ac4f65983ab1638b113bbd",
            "3ddb5d7c8a37425e940624e9e35989b5",
            "92da8904dbc54fb59e0ba9e2bd597aa9",
            "718f6371fc374346897ebdc86fc40d01",
            "ef1480d5e4664bcba60ff67fa7fd61fd",
            "fc18ad0ac4944773a88145045326741c",
            "e58c357bd8414728b0a1a66f9ce74d1c",
            "77bc8fec33144b609c33188719daff27",
            "4d656ae254de4e1698ae26a8b0a079db",
            "d1b5aa6da1d94d4aa4fa6d54235fbf3a",
            "9c7dc5946bae4b8db158867190551a51",
            "c1d28bc73514404487729a45653617ef",
            "6e44a098508b457fba71e61652c6770a",
            "e42f82f8aac34feaa33bc4539cc2718c",
            "69a47a9ce763441b90248747fcae3bb2",
            "61baef8d221a48a48ee6e7ff622b0283",
            "3d56c012fb0c442ca5078d1a89e5fe5a"
          ]
        },
        "outputId": "5c740171-65df-450d-f555-b68bbe2f2836"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_encoder (__main__.TestEncoder) ... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91b5835bd9f44713bfc86f998c60c883"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e58c357bd8414728b0a1a66f9ce74d1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 8.934s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7c2c9869f250>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must use the same tokenizer used in DistilBERT. We will load it as a constant variable and use it throughout the assignment."
      ],
      "metadata": {
        "id": "cv74j7ayvMgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "eua81dK2vHGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "411a73599dfb405e91767d590fac2d74",
            "d58c5254f67f445181989aa3d33c23e1",
            "44e51c944fb541d8b21710b045fddbc2",
            "66321011660f4226afb424567238b509",
            "f40432e01efb4730b6b3d046f9d6b8cf",
            "ee53bf14339241bbb1a9b7a8cd91cee2",
            "37032783bee04bd8b39f82ca09abd4ba",
            "2996b5f67a634cf8b1e1b5f76020ac00",
            "e5a39f786f4a454bb51aa190b193dc0d",
            "a64578aab57c4839b3daccdd802002e4",
            "4e11485052c14ff98bbc05bacc733cda",
            "bb37b61cf4da4f598d6f63382991c3f5",
            "4a992b3355b14c7d8690dbde109d4afc",
            "4884760c58fa4e2b8d849576064eca4f",
            "1242ec1eec42461cb76c068a1645ebde",
            "ea6b4e1c709448fa975b4334c1d9ca3e",
            "83618b47b9b84fcfa58fe7e7a1ca3c7d",
            "aca93d46f3a943e989f5782a141116ce",
            "d72ca8f7dc31443b94c22f6433e8dab2",
            "8b846d804c4241babc29bbc376764c69",
            "6cdd4535aa2a415db2caac1ad5aadae0",
            "273a8912f3fd4d25950c4d825df9409a",
            "413c037d882f4e0cbf66e305d545e398",
            "a23afda0bb3145ff84be0ec49591da47",
            "63d04267dc52473fa3a40aee1bbddeea",
            "613b0cbf87334f0ea17d3f381a2b9a8f",
            "dfb915f9e93c474c8b75ee09c93c3aa8",
            "9bac45bbb5b6485b8d754e0a168fcfe0",
            "d33fb990dffd4e80b64708ec9b0a755d",
            "e7f6496ddb134771ade1471f7d76fb19",
            "4e2416180d124b32a316a2e21eb50278",
            "5977c8f8f8024ea6bf7497e02fc4c385",
            "33dd8241c5d9446eb979a94c8e1b022a"
          ]
        },
        "outputId": "cbd4f194-2923-4c2e-9856-be6ceb36f91c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "411a73599dfb405e91767d590fac2d74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb37b61cf4da4f598d6f63382991c3f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "413c037d882f4e0cbf66e305d545e398"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i1 = 0\n",
        "i2 = 2\n",
        "# Example (batch) encoding\n",
        "texts = [data_train[i1]['gold_context']['title'], data_train[i2]['gold_context']['title']]\n",
        "encoded = distilbert_tokenizer(texts, padding=True, truncation=True, max_length=10, return_tensors='pt')  # Try changing the max length\n",
        "print(encoded.keys())  # input_ids, attention_mask\n",
        "print(encoded['input_ids'].shape)\n",
        "print(encoded['input_ids'])  # Special tokens [CLS] (index 101), [SEP] (index 102), [PAD] (index 0) are automatically inserted upon tokenization\n",
        "print(encoded['attention_mask'])\n",
        "\n",
        "# Example (batch) decoding\n",
        "decoded = distilbert_tokenizer.batch_decode(encoded['input_ids'], skip_special_tokens=False)\n",
        "print(decoded)\n",
        "\n",
        "# BERT-style tokenizers also support appending a second text, as in \"[CLS] text [SEP] another text [SEP]\".\n",
        "second_texts = [data_train[i1]['gold_context']['text'], data_train[i2]['gold_context']['text']]\n",
        "encoded = distilbert_tokenizer(texts, second_texts, padding=True, truncation=True, max_length=15, return_tensors='pt')  # Try changing the max length\n",
        "print('\\n', encoded['input_ids'].shape)\n",
        "print(encoded['input_ids'])\n",
        "print(encoded['attention_mask'])\n",
        "decoded = distilbert_tokenizer.batch_decode(encoded['input_ids'], skip_special_tokens=False)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "zUsPCecdstUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97da7728-42cb-4642-8d48-f1ac6ad1753f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "torch.Size([2, 7])\n",
            "tensor([[  101,  5920,  4686,  1006,  2143,  1007,   102],\n",
            "        [  101, 17266, 26692,  6632,  2278,  4101,   102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1]])\n",
            "['[CLS] suicide squad ( film ) [SEP]', '[CLS] sacroiliac joint [SEP]']\n",
            "\n",
            " torch.Size([2, 15])\n",
            "tensor([[  101,  5920,  4686,  1006,  2143,  1007,   102,  1996,  2143,  1010,\n",
            "          1998,  3660, 24201,  2623,   102],\n",
            "        [  101, 17266, 26692,  6632,  2278,  4101,   102, 17266, 26692,  6632,\n",
            "          2278,  4101,  1996, 17266,   102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "['[CLS] suicide squad ( film ) [SEP] the film, and scott eastwood announced [SEP]', '[CLS] sacroiliac joint [SEP] sacroiliac joint the sac [SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to cut off the input and throw away any text beyond length $T_{\\max}$ for computational reasons. Remember that the runtime is quadratic in the input length for transformers.\n",
        "\n",
        "(You don't want to do this in reality since crucial information might be at the end of a passage!)"
      ],
      "metadata": {
        "id": "xBeXKsyJ6Qg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_encoded = distilbert_tokenizer([example['question'] for example in data_train], padding=True, return_tensors='pt')\n",
        "passages_encoded = distilbert_tokenizer([passage['title'] for passage in KB],\n",
        "                                        [passage['text'] for passage in KB], padding=True, return_tensors='pt')\n",
        "max_question_length = questions_encoded['input_ids'].size(1)\n",
        "max_passage_length = passages_encoded['input_ids'].size(1)\n",
        "print('max question length: {:d}'.format(max_question_length))\n",
        "print('max passage length: {:d}'.format(max_passage_length))\n",
        "\n",
        "MAX_INPUT_LENGTH=100\n",
        "print('OUR max input length: {:d}'.format(MAX_INPUT_LENGTH))"
      ],
      "metadata": {
        "id": "u0MSkMRS6Z_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c7e7a5-ab0e-4df7-b54a-6c33f5e2bb5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max question length: 23\n",
            "max passage length: 210\n",
            "OUR max input length: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting together, here's a function that produces a DistilBERT-initialized dual encoder. We will have an option to tie the query and passage encoders. Encoder tying will clearly be more computationally efficient, but it may not always work as well as not tying."
      ],
      "metadata": {
        "id": "sR2k-vDMh-Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_distilbert_dual_encoder(tie_encoders=False):\n",
        "  query_encoder = DistilBertEncoder()\n",
        "  passage_encoder = query_encoder  if tie_encoders else DistilBertEncoder()\n",
        "  model = DualEncoder(query_encoder, passage_encoder)\n",
        "  return model"
      ],
      "metadata": {
        "id": "Lf-55l3Qp50I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "iOgAv_R9zf3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Stuff"
      ],
      "metadata": {
        "id": "zCQ_DyUoAkE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need a PyTorch dataset to feed our data to a DataLoader. We will reuse the same Dataset class for both the NQ data (which contains questions and passages) and the KB (which contains only passages) since both are just list of samples.\n"
      ],
      "metadata": {
        "id": "9EMtSA1Qztny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data):\n",
        "    self.samples = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.samples[index]"
      ],
      "metadata": {
        "id": "KDXQSzThwpl8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write code to turn a batch of training/evaluation data into torch-friendly tensors. A sample has question, gold_context, and optionally hard_negative."
      ],
      "metadata": {
        "id": "GVQvnJuB_oij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorize_contrastive(samples, use_hard_negative=False):\n",
        "  queries = []\n",
        "  labels = []\n",
        "  titles = []\n",
        "  texts = []\n",
        "\n",
        "  for sample in samples:\n",
        "    queries.append(sample['question'])\n",
        "    labels.append(len(titles))  # Index of correct context for the question\n",
        "\n",
        "    titles.append(sample['gold_context']['title'])\n",
        "    texts.append(sample['gold_context']['text'])\n",
        "\n",
        "    if use_hard_negative and 'hard_negative' in sample:\n",
        "      titles.append(sample['hard_negative']['title'])\n",
        "      texts.append(sample['hard_negative']['text'])\n",
        "\n",
        "  queries = distilbert_tokenizer(queries, padding=True, truncation=True, max_length=MAX_INPUT_LENGTH, return_tensors='pt')\n",
        "  passages = distilbert_tokenizer(titles, texts, padding=True, truncation=True, max_length=MAX_INPUT_LENGTH, return_tensors='pt')\n",
        "\n",
        "  Q, Q_mask = queries['input_ids'], queries['attention_mask']  # (batch_size, question_length)\n",
        "  P, P_mask = passages['input_ids'], passages['attention_mask'] # (batch_size, passage_length) if no hard negatives else (2*batch_size, passage_length)\n",
        "  labels = torch.LongTensor(labels)  # (batch_size,) elements in [0, batch_size) or [0, 2 * batch_size)\n",
        "\n",
        "  return Q, Q_mask, P, P_mask, labels"
      ],
      "metadata": {
        "id": "1cmXY9lTsMQ6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also write code to tensorize a batch of the KB, which only consists of passages."
      ],
      "metadata": {
        "id": "ciZ3LwfTAsgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorize_KB_batch(passages):\n",
        "  titles = [passage['title'] for passage in passages]\n",
        "  texts = [passage['text'] for passage in passages]\n",
        "\n",
        "  # We must make sure we encode passages the same way as we do in training.\n",
        "  passages = distilbert_tokenizer(titles, texts, padding=True, truncation=True, max_length=MAX_INPUT_LENGTH, return_tensors='pt')\n",
        "  P, P_mask = passages['input_ids'], passages['attention_mask']  # (batch_size, passage_length)\n",
        "  return P, P_mask"
      ],
      "metadata": {
        "id": "xD5rzJZ82jTU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recall"
      ],
      "metadata": {
        "id": "4pFQ2DRuBCwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming a trained dual encoder, we can embed all KB passages offline."
      ],
      "metadata": {
        "id": "FGKQlX8R1gEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_KB(KB, model, batch_size, device='cuda', disable_tqdm=True):\n",
        "  model.eval()\n",
        "  collate_fn = lambda samples: tensorize_KB_batch(samples)\n",
        "  loader = DataLoader(MyDataset(KB), batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "  passage_embeddings = []\n",
        "  with torch.no_grad():\n",
        "    for P, P_mask in tqdm(loader, disable=disable_tqdm):\n",
        "      _, Y = model(Q=None, Q_mask=None, P=P.to(device), P_mask=P_mask.to(device))\n",
        "      passage_embeddings.append(Y.cpu())\n",
        "  passage_embeddings = torch.cat(passage_embeddings, 0)  # (|KB|, dim)\n",
        "  return passage_embeddings"
      ],
      "metadata": {
        "id": "MeSmXtIG1qcd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a real-world implementation the passage embedding computation must be done in parallel using many GPUs. Assuming that the passages in the KB are already embedded, at test time we only need to embed the questions and perform maximum inner product search (MIPS). Let's write code for embedding questions."
      ],
      "metadata": {
        "id": "wxQNU5-B7TWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_questions(data, model, batch_size, device='cuda', disable_tqdm=True):\n",
        "  model.eval()\n",
        "  collate_fn = lambda samples: tensorize_contrastive(samples, use_hard_negative=False)\n",
        "  loader = DataLoader(MyDataset(data), batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "  question_embeddings = []\n",
        "  with torch.no_grad():\n",
        "    for Q, Q_mask, _, _, _ in tqdm(loader, disable=disable_tqdm):\n",
        "      X, _= model(Q=Q.to(device), Q_mask=Q_mask.to(device), P=None, P_mask=None)\n",
        "      question_embeddings.append(X.cpu())\n",
        "  question_embeddings = torch.cat(question_embeddings, 0)  # (|val|, d)\n",
        "  return question_embeddings"
      ],
      "metadata": {
        "id": "1OdGob2yjvIh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write code to compute top-$k$ candidate passages per question."
      ],
      "metadata": {
        "id": "uUXQzVFAk-xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topk_candidates(KB, data, model, batch_size, num_candidates, device='cuda', disable_tqdm=True):\n",
        "  passage_embeddings = embed_KB(KB, model, batch_size, device=device, disable_tqdm=disable_tqdm)\n",
        "  question_embeddings = embed_questions(data, model, batch_size, device=device, disable_tqdm=disable_tqdm)\n",
        "  # Performing MIPS between the questions and the passage embeddings\n",
        "  scores = torch.matmul(question_embeddings, passage_embeddings.T)\n",
        "  values, indices = torch.topk(scores, num_candidates)\n",
        "  results = []\n",
        "  for i in range(indices.size(0)):\n",
        "    result = {'question': data[i]['question'],\n",
        "              'answers': data[i]['answers'],\n",
        "              'gold_title': data[i]['gold_context']['title'],\n",
        "              'topk_titles': [KB[candidate_index]['title'] for candidate_index in indices[i]],\n",
        "              'topk_indices': indices[i].tolist(),\n",
        "              'topk_values': values[i].tolist(),\n",
        "              }\n",
        "    results.append(result)\n",
        "  return results"
      ],
      "metadata": {
        "id": "3NdtA44K62KA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given results, we can compute Recall@$k$ for various values of $k$."
      ],
      "metadata": {
        "id": "ETFULMtQsi_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_retrieval_accuracy(results, k_values=[1, 5, 10, 20, 100]):\n",
        "  k_num_correct = {}\n",
        "  for k in k_values:\n",
        "    k_num_correct[k] = 0\n",
        "  for result in results:\n",
        "    rank_min = float('inf')\n",
        "    for rank, title in enumerate(result['topk_titles']):\n",
        "      if title == result['gold_title']:\n",
        "        rank_min = rank\n",
        "        break\n",
        "    for k in k_values:\n",
        "      if rank_min < k:\n",
        "        k_num_correct[k] += 1\n",
        "\n",
        "  num_queries = len(results)\n",
        "  k_accuracy = {k: num_correct / num_queries * 100. for (k, num_correct) in k_num_correct.items()}\n",
        "  k_accuracy['num_queries'] = num_queries\n",
        "  return k_accuracy"
      ],
      "metadata": {
        "id": "oBpikItTqrU5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try evaluating the recall of an untrained dual encoder. Using max length 100 (thus failing to look at all of a passage) for computational reasons."
      ],
      "metadata": {
        "id": "xBHvD3x950iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEval(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    self.places = 4\n",
        "\n",
        "  def test_eval(self):\n",
        "    model = make_distilbert_dual_encoder(tie_encoders=False).to('cuda')\n",
        "    results = get_topk_candidates(KB, data_val, model, batch_size=600, num_candidates=100, device='cuda', disable_tqdm=True)\n",
        "    k_accuracy = topk_retrieval_accuracy(results, k_values=[1, 5, 10, 20, 100])\n",
        "    self.assertAlmostEqual(k_accuracy[1], 6.20, places=self.places)\n",
        "    self.assertAlmostEqual(k_accuracy[5], 11.30, places=self.places)\n",
        "    self.assertAlmostEqual(k_accuracy[10], 14.50, places=self.places)\n",
        "    self.assertAlmostEqual(k_accuracy[20], 19.20, places=self.places)\n",
        "    self.assertAlmostEqual(k_accuracy[100], 37.20, places=self.places)\n",
        "    clear_from_gpu(model)\n",
        "\n",
        "unittest.main(TestEval(), argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "s9pa7yJE2qit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ba430a-c379-45a8-8724-60687a8804d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_eval (__main__.TestEval) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 8.697s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7c2c90d0ef20>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model is not finetuned, the recall is not good. Let's see if we can improve it by NCE training."
      ],
      "metadata": {
        "id": "2p3rFwLYsyt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise Contrastive Estimation (NCE)"
      ],
      "metadata": {
        "id": "0CbYxiPJxZC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of a dual encoder is to retrieve the right passage (i.e., context) $p \\in \\mathcal{V}^+$ for a given question $q \\in \\mathcal{V}^+$. The dual encoder defines the score function\n",
        "\n",
        "$$\n",
        "\\mathrm{score}_\\theta(q,p) = \\mathrm{QueryEncoder}_\\theta(q) \\cdot \\mathrm{PassageEncoder}_\\theta(p)\n",
        "$$\n",
        "\n",
        "(In our case, both encoders are the same DistilBERT.) Thus at test time, given any query $q$ the retriever outputs $p^\\star$ with $\\max \\mathrm{score}_\\theta(q,p)$. The usual way to train a model is by minimizing the empirical cross-entropy loss\n",
        "$$\n",
        "\\hat{J}(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(\\mathrm{score}_\\theta(q_i,p_i))}{\\sum_{p \\in \\textrm{KB}} \\exp(\\mathrm{score}_\\theta(q_i,p))}\n",
        "$$\n",
        "The problem is that KB is generally too large to compute the normalization term. In NCE, we instead minimize\n",
        "$$\n",
        "\\hat{J}_{\\text{nce}}(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(\\mathrm{score}_\\theta(q_i,p_i))}{\\sum_{p \\in \\{p_i\\} \\cup \\mathcal{N}_i} \\exp(\\mathrm{score}_\\theta(q_i,p))}\n",
        "$$\n",
        "where $\\mathcal{N}_i$ is a much smaller set of **negative** (i.e., incorrect) passages for question $i$. The choice of negative examples is very important for both theoretical and empirical reasons. See [this note](http://karlstratos.com/notes/nce.pdf) for details.\n",
        "\n",
        "A naive way to implement NCE is to prepare a set of negative passages for every question in each epoch. A more computationally efficient way to simulate this is **in-batch** negative sampling, which uses the fact that other *gold* passages in the same batch can be treated as negative examples for the given question. It's also possible to sneak in so-called \"hard negatives\". That is, each question, in addition to a gold passage, can be associated with an incorrect but correct-looking passages. If we include such hard negatives in the in-batch negative sampling scheme, the model will be trained to distinguish the gold passage from the hard negative. See the following illustration from  [Maillard et al., 2021](https://arxiv.org/pdf/2101.00117.pdf):\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src='https://drive.google.com/uc?id=1qK3t0NnSEqTjM17J6hYnmSNTtEwSQWb8' width=\"600\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "Mbrans4tyT0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now write a function that computes the in-batch NCE loss.\n"
      ],
      "metadata": {
        "id": "LWf1cxt83q_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_in_batch_contrastive_loss(model, batch, device='cuda'):\n",
        "  Q, Q_mask, P, P_mask, labels  = [tensor.to(device) for tensor in batch]\n",
        "  # Get embeddings from the model\n",
        "  Q_embeddings = model.query_encoder(Q, attention_mask=Q_mask)\n",
        "  P_embeddings = model.passage_encoder(P, attention_mask=P_mask)\n",
        "\n",
        "  # Calculate the score matrix by taking the dot product of Q and P embeddings\n",
        "  scores = torch.matmul(Q_embeddings, P_embeddings.T)\n",
        "\n",
        "  # Calculate the NCE loss\n",
        "  loss = F.cross_entropy(scores, labels)\n",
        "\n",
        "  # Calculate the number of correct predictions\n",
        "  num_correct = (scores.argmax(dim=1) == labels).sum()\n",
        "  return loss, num_correct"
      ],
      "metadata": {
        "id": "i3r-tC7Vo6s6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestGetInBatchContrastiveLoss(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    set_seed(42)\n",
        "    self.samples = [\n",
        "        {'question': 'a b c d', 'gold_context': {'title': 'e f g', 'text': 'the dog saw the cat'}, 'hard_negative': {'title': 'h i', 'text': 'the cat saw the dog'}},\n",
        "        {'question': '1 2 3 4', 'gold_context': {'title': '5 6 7', 'text': 'foo bar'}, 'hard_negative': {'title': '8 9', 'text': 'z y'}},\n",
        "        {'question': 'A B C D', 'gold_context': {'title': 'E F G', 'text': 'the DOG'}, 'hard_negative': {'title': 'H I', 'text': 'GOD'}}\n",
        "    ]\n",
        "    self.places = 4\n",
        "    self.device = 'cpu'\n",
        "\n",
        "  def test_get_in_batch_contrastive_loss(self):\n",
        "    model = make_distilbert_dual_encoder()\n",
        "    batch = tensorize_contrastive(self.samples, use_hard_negative=True)\n",
        "    loss, num_correct = get_in_batch_contrastive_loss(model, batch, device=self.device)\n",
        "\n",
        "    self.assertAlmostEqual(loss.item(), 1.1979, places=self.places)\n",
        "    self.assertEqual(num_correct.item(), 1)\n",
        "\n",
        "unittest.main(TestGetInBatchContrastiveLoss(), argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "V1PspNucmmXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94c6e08-26b3-4e61-fb28-73a95c3de622"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_get_in_batch_contrastive_loss (__main__.TestGetInBatchContrastiveLoss) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 2.579s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7c2c986cce20>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "UqHjyyAo6FBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the standard setting for finetuning BERT."
      ],
      "metadata": {
        "id": "uT1tYLQj6IB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def configure_optimization(model, num_train_steps, num_warmup_steps, lr, weight_decay=0.01):\n",
        "  # Copied from: https://huggingface.co/transformers/training.html\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "  optimizer_grouped_parameters = [{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                                   'weight_decay': weight_decay},\n",
        "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                                   'weight_decay': 0.}]\n",
        "  optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n",
        "  return optimizer, scheduler"
      ],
      "metadata": {
        "id": "X0HxHrdIqdhB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nce(model, batch_size, epochs, lr, weight_decay=0.01, use_hard_negative=False, disable_tqdm=True):\n",
        "  set_seed(42)\n",
        "  collate_fn = lambda samples: tensorize_contrastive(samples, use_hard_negative=use_hard_negative)\n",
        "  loader_train = DataLoader(MyDataset(data_train), batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  loader_val = DataLoader(MyDataset(data_val), batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "  num_train_steps = len(loader_train) * epochs\n",
        "  optimizer, scheduler = configure_optimization(model, num_train_steps, int(0.1 * num_train_steps), lr)\n",
        "\n",
        "  best_r5 = 0.  # Let's use Recall@5 for model selection\n",
        "  best_state_dict = None\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loss_total = 0.\n",
        "\n",
        "    for batch in tqdm(loader_train, disable=disable_tqdm):\n",
        "      loss, num_correct = get_in_batch_contrastive_loss(model, batch, device='cuda')\n",
        "      loss_total += loss.item()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "    loss_avg = loss_total / len(loader_train)\n",
        "    results = get_topk_candidates(KB, data_val, model, batch_size=600, num_candidates=100, device='cuda', disable_tqdm=True)\n",
        "    k_accuracy = topk_retrieval_accuracy(results, k_values=[1, 5, 10, 20, 100])\n",
        "    info = 'Epoch {:3d} | average loss {:8.4f} | R@1 {:5.2f} | R@5 {:5.2f} | R@10 {:5.2f} | R@20 {:5.2f} | R@100 {:5.2f}'.format(\n",
        "        epoch, loss_avg, k_accuracy[1], k_accuracy[5], k_accuracy[10], k_accuracy[20], k_accuracy[100])\n",
        "    if k_accuracy[5] > best_r5:\n",
        "      best_r5 = k_accuracy[5]\n",
        "      best_state_dict = copy.deepcopy(model.state_dict())\n",
        "      info += ' <-------------- (saved)'\n",
        "    print(info)\n",
        "\n",
        "  if best_state_dict is not None:\n",
        "    model.load_state_dict(best_state_dict)"
      ],
      "metadata": {
        "id": "L76v50q5mRuD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_distilbert_dual_encoder(tie_encoders=False).to('cuda')\n",
        "train_nce(model, 128, 5, 1e-4, disable_tqdm=True)"
      ],
      "metadata": {
        "id": "ngKK-IUYPspG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6d65c8-880b-439e-b0d6-2647f07cb80b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | average loss  10.1849 | R@1 26.10 | R@5 49.20 | R@10 56.70 | R@20 63.40 | R@100 81.50 <-------------- (saved)\n",
            "Epoch   1 | average loss   2.2911 | R@1 40.20 | R@5 65.80 | R@10 71.80 | R@20 79.10 | R@100 92.40 <-------------- (saved)\n",
            "Epoch   2 | average loss   1.1167 | R@1 53.40 | R@5 75.70 | R@10 82.40 | R@20 86.10 | R@100 95.10 <-------------- (saved)\n",
            "Epoch   3 | average loss   0.6704 | R@1 60.40 | R@5 82.30 | R@10 86.30 | R@20 90.50 | R@100 96.90 <-------------- (saved)\n",
            "Epoch   4 | average loss   0.4350 | R@1 62.60 | R@5 83.10 | R@10 87.60 | R@20 91.40 | R@100 97.70 <-------------- (saved)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that's substantially better than an untrained dual encoder. Let's see if we can improve this even further by hard negative mining."
      ],
      "metadata": {
        "id": "03fc6cq3V4zY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard Negative Mining"
      ],
      "metadata": {
        "id": "CWdHESrVWBib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of hard negatives is flexible (e.g., you can use an off-the-shelf retriever like BM25 and take the top incorrect candidate). But they can be mined from the trained model itself."
      ],
      "metadata": {
        "id": "K_1SIxI8WDJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_topk_candidates(KB, data_train, model, batch_size=600, num_candidates=2, device='cuda', disable_tqdm=True)"
      ],
      "metadata": {
        "id": "JZAxvUZTWWfS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "for result in random.sample(results, 7):\n",
        "  print('Gold:         ', result['gold_title'])\n",
        "  print('1st candidate:', result['topk_titles'][0])\n",
        "  print('2nd candidate:', result['topk_titles'][1])\n",
        "  print()"
      ],
      "metadata": {
        "id": "v9GRztJLWfu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6672dfef-e13c-470f-ea4f-37297acf992d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gold:          Classical conditioning\n",
            "1st candidate: Classical conditioning\n",
            "2nd candidate: Wilhelm Wundt\n",
            "\n",
            "Gold:          New York Mets\n",
            "1st candidate: New York Mets\n",
            "2nd candidate: 2016 World Series\n",
            "\n",
            "Gold:          Water distribution on Earth\n",
            "1st candidate: Water distribution on Earth\n",
            "2nd candidate: Earliest known life forms\n",
            "\n",
            "Gold:          Kirby's Epic Yarn\n",
            "1st candidate: Kirby's Epic Yarn\n",
            "2nd candidate: The Incredibles\n",
            "\n",
            "Gold:          Little Shop of Horrors (film)\n",
            "1st candidate: Little Shop of Horrors (film)\n",
            "2nd candidate: Hotel Transylvania\n",
            "\n",
            "Gold:          I'll Be There (The Jackson 5 song)\n",
            "1st candidate: Ten Years After\n",
            "2nd candidate: Fooled Around and Fell in Love\n",
            "\n",
            "Gold:          Chorion\n",
            "1st candidate: Meninges\n",
            "2nd candidate: Cytokinesis\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that in many cases the correct passage is ranked at the top. We will take the 2nd passage as a hard negative in that case. If the top passage is not correct, then that's our hard negative for that example."
      ],
      "metadata": {
        "id": "DyF1FUGYX2Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, example in enumerate(data_train):\n",
        "  topk_result = results[i]\n",
        "\n",
        "  # Checking if the top passage is the correct one\n",
        "  if topk_result['topk_titles'][0] == example['gold_context']['title']:\n",
        "      # If yes, take the 2nd passage as a hard negative\n",
        "      i_hard = topk_result['topk_indices'][1]\n",
        "  else:\n",
        "      # If not, take the top passage as a hard negative\n",
        "      i_hard = topk_result['topk_indices'][0]\n",
        "\n",
        "  # Populating the hard_negative in the example\n",
        "  example['hard_negative'] = KB[i_hard]  # Populating hard_negative in examples allows tensorize_contrastive to use hard negatives."
      ],
      "metadata": {
        "id": "G147EwGOYFGL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try training a dual encoder again, this time using $2B-1$ negative examples in NCE rather than $B-1$ including $B$ hard negatives. We can continue training the existing model, but we are limited in GPU memory here so we'll just start from the raw DistilBERT model again."
      ],
      "metadata": {
        "id": "T2Suz0VQZShU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_from_gpu(model)\n",
        "model = make_distilbert_dual_encoder(tie_encoders=False).to('cuda')\n",
        "train_nce(model, 64, 5, 1e-4, use_hard_negative=True, disable_tqdm=True)  # Need a smaller batch size since each batch has dimension B * 2B now"
      ],
      "metadata": {
        "id": "AzMe3QBrZie-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac875299-f96c-4e53-a310-28c49a8f8c4f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | average loss   8.0642 | R@1 31.60 | R@5 54.20 | R@10 62.60 | R@20 69.50 | R@100 85.80 <-------------- (saved)\n",
            "Epoch   1 | average loss   1.8812 | R@1 51.50 | R@5 74.20 | R@10 80.20 | R@20 85.50 | R@100 94.90 <-------------- (saved)\n",
            "Epoch   2 | average loss   0.7474 | R@1 62.40 | R@5 84.20 | R@10 88.50 | R@20 91.60 | R@100 98.00 <-------------- (saved)\n",
            "Epoch   3 | average loss   0.3230 | R@1 65.50 | R@5 85.90 | R@10 90.20 | R@20 93.10 | R@100 98.30 <-------------- (saved)\n",
            "Epoch   4 | average loss   0.1838 | R@1 65.70 | R@5 86.50 | R@10 90.20 | R@20 93.70 | R@100 98.30 <-------------- (saved)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that seems to converge faster and to a better model. You can iterate (i.e., keep doing hard negative mining). In practice, you can come up with various training schemes using hard negatives. For example \"refresh\" hard negatives every epoch.\n",
        "\n",
        "Hard negative mining was proposed for entity retrieval ([Gillick et al., 2019](https://aclanthology.org/K19-1049.pdf)). Also see [this paper](https://arxiv.org/pdf/2104.06245.pdf) for an anlysis of hard negative mining."
      ],
      "metadata": {
        "id": "eM612guXaxM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "TODO: Fill in the following table with the *best* validation recalls you could get without and with hard negative mining. You are encouraged to try different hyperparameter values like doing sufficiently many epochs to converge, using different learning rates and batch sizes.\n",
        "\n",
        "| Model   | Recall@1  |   Recall@5 | Recall@10 | Recall@20 |  Recall@100 |\n",
        "| :---:   | :---:     |:---:       | :---:     | :---:     | :---:       |\n",
        "| Untrained |  6.20 |  11.30     | 14.50     | 19.20     |  37.20  |\n",
        "| NCE       |  62.60  | 83.10 | 87.60 | 91.40 | 97.70 |\n",
        "| NCE+hard  |   65.70 | 86.50| 90.20 |93.70 | 98.30|"
      ],
      "metadata": {
        "id": "5DU4o-UrrEY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "nv46tV6qcblm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learnable retrieval is generally useful for a variety of tasks. (E.g., finding $k$ documents that are most \"similar\" to a query document is of intrinsic interest.)\n",
        "\n",
        "This assignment focused on a particular retrieval setting, namely retrieving evidence passages for QA. This is a so-called \"retrieve-then-read\" pipeline, where given a question $q$ the model retrieves top-$k$ passage candidates and then reads them to produce the final answer. The answer module can be any language model (encoder-decoder or decoder-only) that conditions on the question and passages and predicts an answer string. A popular model is [Fusion-in-Decoder](https://arxiv.org/pdf/2007.01282.pdf). It's simply a standard encoder-decoder model (e.g., you can use T5) except that you preprocess the input to perform full self-attention over $(q, p)$ pairs separately where $p$ is a single passage, but the decoder conditions on *all* of the encoded tokens. This can scale very well in the number of passages to condition on. If you naively concatenate $(q, p_1 \\ldots p_k)$ and use it as the input to a transformer encoder, it will not scale well. With FiD, the model can condition on more than a hundard candidate passages.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src='https://drive.google.com/uc?id=1MtZVfA2vzts29hK5DvjeuLNBGPlNUOLa' width=\"1000\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "xJCf3sQXcc7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, in this assignment we assume that annotations for retrieval are available. I.e., somebody annotated the gold passage for each question. This is expensive and it's becoming more and more clear that it is not necessary. There are methods to learn to retrieve by doing a downstream task like language modeling and QA, without explicit retrieval annotations. They are based on marginalization (e.g., [REALM](https://arxiv.org/pdf/2002.08909.pdf), [EMDR2](https://arxiv.org/pdf/2106.05346.pdf)) or, more compellingly, distillation (e.g., [FiD-KD](https://arxiv.org/pdf/2012.04584.pdf))."
      ],
      "metadata": {
        "id": "JzQQ462ZfUbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, instead of learning a retriever, we can use existing search engines like Google as an API for the model to use. This is exactly what you would want if your retrieval needs are consistent with what a search engine provides (e.g., [Sparrow](https://arxiv.org/pdf/2209.14375.pdf)). However, it's not what you want if you need your retriever to be specialized in some unique task not suitable for generic search engines (e.g., [entity linking](https://arxiv.org/pdf/2110.02369.pdf))."
      ],
      "metadata": {
        "id": "vqWFXjchg1wW"
      }
    }
  ]
}